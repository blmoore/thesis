\documentclass[a4paper,11pt,oneside]{book}

% packages 
\usepackage{arsclassica}    % fancy layout
\usepackage[english]{babel}\addto{\captionsenglish}{\renewcommand{\bibname}{References}}
\usepackage{caption}         % figure captions
\usepackage[square,numbers,super,sort&compress]{natbib}  % bibliography style
\usepackage[cc]{titlepic}    % enable logo on title page
\usepackage{graphicx}       % logo related

\usepackage{standalone}

\captionsetup{format=plain}

% bibliography
\bibliographystyle{../thesis}

% title setup
\title{ \vspace{3in} Unravelling higher order genome organisation {\small [working
    title]} \\ \vspace{2em} {\large {\bf Results 2} Predictive modelling } }
\author{Benjamin L. Moore}
\titlepic{\vspace{2.2in} \includegraphics[width=\textwidth]{/Users/benmoore/hvl/1yrReport/figs/igmm.png}}

\begin{document}

\maketitle

\chapter{Integrative modelling as a tool to explore biological systems}
\vspace{2em}

\section{Introduction}
Large-scale chromatin data has recently been produced by multiple
consortia, most notably the ENCODE\cite{Gerstein2012} and NIH
Roadmap Epigenomics\cite{Bernstein2010} projects. The breadth and depth of this new
data offers unprecedented opportunities to further our understanding
regarding the fundamental biology of the chromatin landscape. While many histone
modifications can now be quantified experimentally,\cite{Nikolov2012, Sajan2012, Ernst2011} an integrated
understanding of general mechanisms underlying the cause or effect of
these marks lags behind. A 2011 opinion piece asked
the question ``Histone modification: cause or
cog?''\cite{Henikoff2011} and speculated that nucleosome modifications
could be by-products of transcription machinery, as opposed to
the ``histone code'' hypothesis which suggests that histone
modifications are placed to direct alterations in chromatin
state. This latter hypothesis is often tacitly invoked in the
chromatin literature, wherein a mark may be described as
``repressive'' or ``activating'' despite only the observation of a
correlative relationship.\cite{Henikoff2011} Similarly, the interplay
between locus-level factors and higher-order organisation of
chromatin, while known the be an important factor in
transcription, remains poorly understood mechanisatically.\cite{Li2011} 
% Colin's sentence:
However, the recent flood of data from high throughput sequencing technologies have provided fascinating new glimpses of the ways chromatin and transcription are functionally related.

Recent studies have shown convincingly that local chromatin state
measurements can accurately predict expression levels of genes on a
genome-wide basis. Tippmann \emph{et
  al.},\cite{Tippmann2012} designed a linear model to predict
steady-state mRNA levels in mouse (\emph{Mus musculus}) embryonic stem
cells based on just four predictors: 3 histone modifications
(H3K36me3, H3K4me2 and H3K27me3) and
Pol-II occupancy. Remarkably, the linear model was found to explain
84.6\% of an estimated 91\% maximal variance that could be explained
(as calculated through a detailed determination of noise). An additional
finding of this study was that mRNA half-life and microRNA mediated
transcript degradation both had relatively minor influence on
steady-state mRNA levels, with the authors concluding that``the lion's
share of regulatory contribution is at the level of mRNA synthesis and
predictable from chromatin alone.''\cite{Tippmann2012} An independent
study used a similar regression modelling approach to chromatin
and transcription factor data and again
concluded that models built with histone modifications and chromatin
accessibility data were almost as accurate as those which also
included binding data for 12 transcription factors.\cite{McLeay2012a} 

A recent key study from the ENCODE consortium used chromatin (ChIP-seq) datasets to predict gene expression in a range
of cell types as measured by a variety of experimental techniques.\cite{Dong2012} The authors here developed a
two-stage model which first attempts to classify each transcription
start site (TSS) into an `on' or `off' state using a powerful ensemble
classifier technique called Random Forests (RF). The second stage of the
model used the same range of histone modifications as regressors in a
simple linear modelling framework to quantify predicted
expression. This approach proved very successful, producing a
median Pearson correlation coefficient ($r$) between predicted and
empirical expression levels using 10-fold cross-validation of
$0.83$ across all cell lines and expression level
technologies.\cite{Dong2012} Additionally, this study highlighted cap
analysis of gene expression (CAGE) as the 
technology, relative to RNA-Seq and RNA-PET, which produced the most
predictable expression response. CAGE uses 5$^\prime$ capped transcripts to
generate short, specific tags which precisely identify TSS positions as well as
quantifying the abundance of a given transcript.\cite{Shiraki2003, Kodzius2006}

These recent publications highlight the importance and relevance of
advancing our understanding of chromatin biology through a model-based
approach. Each of these existing models however, treats expression
levels as stationary outcome in each cell type and ignores any temporal
dynamics. The huge amount of novel timecourse CAGE data 
produced by the FANTOM5 consortium\cite{fantom5} puts us in an ideal
position to investigate how chromatin influences transcription beyond a
simple single-point response and move towards a more complete
understanding of the drivers of transcriptional flux.

\section{Reproducing Dong \emph{et al.} }

Following on from Dong \emph{et al.},\cite{Dong2012} I first
reimplemented the published ENCODE modelling framework to ensure I could
replicate their results. In doing so I was also able to analyse the
strengths and caveats of their approach; surprisingly the two-step classification
then regression (firstly assessing a gene as `on' or `off' and then
predicting its expression level) added little additional accuracy relative to a simple
linear regression model 
(Fig. \ref{fig:TwoStepvsSimple}). 

\begin{figure}
\begin{center}
\includegraphics[width=.7\textwidth]{figs/improvedDongPlot.png}
\captionsetup{width=\textwidth}
\caption{Comparison of classification-regression model (\emph{upper})
  with simple linear regression model (\emph{lower}) recalculated following Dong
  \emph{et al.}\cite{Dong2012} Scatterplots of predicted against empirical
  $\log_2$ reads per million (RPM) expression values for both methods are shown (\emph{left})
  along with frequency distributions of predicted and observed
  expression levels (\emph{right}). Scatterplots are annotated with
  Pearson's correlation coefficient (\emph{r}) and the root mean
  squared error (RMSE); the black trendlines describe $y =
  x$. Following 10-fold cross validation, overall correlation
  coefficients were: linear model $0.87 \pm 1.77 \times 10^{-5} $;
  Two-step model $0.872 \pm 9.89 \times 10^{-5} $. All correlations
  were statistically significant with $p < 1 \times 10^{-15}$ under the assumption of a
  $t$-distributed $r$ with $d.f. = 7998$.
}\label{fig:TwoStepvsSimple}
\end{center}
\end{figure} 

An innovative element of Dong \emph{et al.}'s modelling approach is the `bestbin' method of matching
chromatin measurements to the expression of a given TSS. This strategy
first bins normalised signal intensities into $40 \times 100$ bp bins
encompassing 4 kbp around the TSS, and adds an additional bin representing the remaining
gene body. Then the correlation between
the signal of a given mark and the expression of a TSS across all
genes is measured --- the bin producing the highest correlation is
designated as the `bestbin' and that bin's normalised ChIP-seq signal intensity in then
taken forward for the full model. This was shown to raise
the correlation (between predicted and observed expression) by 0.1 in
the simple regression model, an increase in accuracy of almost $13\%$,
relative to simply taking the average value across all
bins.\cite{Dong2012} 

\begin{figure}
\begin{center}
\includegraphics[width=\textwidth]{figs/Dong_relImp_Horiz.pdf}
\captionsetup{width=.9\textwidth}
\caption{Relative importance metrics for variables in both the
  classification (\emph{left}) and regression (\emph{right}) stages of
  my reimplementation of Dong \emph{et al.}'s two-step
  model.\cite{Dong2012} The additional variable `ReplicationTiming'
  shows the influence of $\log_2 (early/late)$ replication timing ratio measured in the BG02 ESC cell type;\cite{Ryba2010}
  H1 hESC data was not available but these higher-order measurements
  appear to be largely conserved across cell-types.\cite{Chambers2012}
  For details of CAR $R^2$ decomposition, see Zuber and Strimmer (2010).\cite{Zuber2011}
}\label{fig:relimp}
\end{center} 
\end{figure} 

I attempted to improve the accuracy of predicted expression values
produced by Dong \emph{et al.} through two methods: increasing the number of informative
regressors and increasing the complexity of the model by adding
interaction terms and/or non-linear components. While Dong \emph{et
  al.} included broad coverage of different histone modifications,
they did not investigate the impact of higher-order
chromatin data. For this reason, I matched the TSS positions used in
Dong \emph{et al.} with previously-published genome-wide replication
timing ratios measured in BG02 ESCs.\cite{Ryba2010} I then used these values as an additional
regressor in both the two-step classification regression model and the
simple linear model but saw no significant improvement in either
model's accuracy. The reasons for this are likely that the 
data were relatively low-resolution (1 megabase blocks), from a
imperfectly matched cell line and also that
the Dong \emph{et al.} model is already achieving such accurate
results that they must already be accounting for most of the maximal
explainable variance in gene expression given experimental and
biological noise. With this in mind, additional regressors would be
expected to yield diminishing returns. However, on closer examination,
the replication timing data
appeared only slightly more informative than the control ChIP-seq input
measurements when evaluated with relative importance metrics
(Fig. \ref{fig:relimp}), implying that large-scale chromatin domains
and long range interactions
do not have significant influence on the expression of the genes resident within them. It would
be of interest to investigate this further should more detailed higher order
data become available. For example Hi-C interaction matrices have been
calculated in the H1 cell line\cite{Dixon2012} and these could be
compressed to principle component eigenvectors as has been done with
other cell lines.\cite{Lieberman2011}

\section{Modelling FANTOM5 CAGE timecourse data}
Using unpublished FANTOM5 data and the approach established above, I next attempted to model gene
expression at timepoint zero ($t_0$) of a differentiation timecourse of Human
H1 embryonic stem cells (H1 hESC) to CD34+ hematopoietic stem
cells.

% How did I do this?:
% 1) Take robustly mapped CAGE clusters -> Entrez Gene IDs
% 2) Select the strongest expressed CAGE cluster
% 3) Either a) use this as a TSS or b) map to closest annotated TSS
The first stage of the analysis was to map each CAGE cluster to a
representative TSS. FANTOM5 robust gene mapping\cite{fantom5}
provided corresponding Entrez Gene IDs for gene-associated CAGE
clusters, and I selected the most expressed cluster to represent the
expression level of its mapped gene. I then compared these to Ensembl
TSS annotations (v69) and
discarded those tag clusters centered on a point $>50$ bp from an annotated
TSS associated with the mapped Entrez Gene ID, thereby removing enhancers and other non-genic transcribed
regions.

Next I retrieved a number of genome-wide histone modification datasets
from the ENCODE and
NIH Roadmap consortia which were measured in H1 hESC cells, taking these to be
reflections of the chromatin state $t_0$. I implemented the
previously-described `bestbin' strategy\cite{Dong2012} to objectively
select the most-correlated binned signal for each chromatinH1 hESC
mark. Additionally, I analysed the stability of chosen bestbins by
calculating them on 200 sets of 1000 randomly selected TSS samples (with each sample
representing approximately $8\%$ of the dataset) and the result is
shown in Figure \ref{fig:bestbin}.

\begin{figure}
\begin{center} 
\includegraphics[width=.9\textwidth]{figs/bestbinSummary.pdf}
\captionsetup{width=\textwidth}
\caption{Distributions of bestbin locations relative to the
  TSS. Bestbins were selected for normalised ChIP-seq
  signal intensities for 10 histone marks, the
  H2A.Z histone variant, Hdac6 histone deacetylase, Dnase
  hypersensitivity and a ChIP-seq input chromatin control. Bins analysed
  extended 2 Kb flanking the TSS, but more distal bins were
  never selected and hence are not shown. `Whole gene` represents the
  averaged signal intensity from TSS to transcript end site, as
  defined by Ensembl Genes v69.
}\label{fig:bestbin}
\end{center}
\end{figure} 

This result shows that bestbin selections are often consistent, indicating there are predictably informative regions
relative to a TSS for each chromatin factor (Fig. \ref{fig:bestbin}). Furthermore, the selected
bestbins match known biological mechanisms; for example the H3K36me3
mark's bestbin is consistently the whole gene measurement and this
mark is known to be enriched in actively transcribed
exons.\cite{Tippmann2012, Kolasinska-Zwierz2009, Schaft2003} 

Having matched a variety of genome-wide H1 hESC chromatin datasets
to the FANTOM5 timecourse expression data, I then built a regression
model using a Random Forest (RF) approach.\cite{Breiman2001} This method outperforms a simple
linear model in my initial comparisons and is able to capture non-linear relationships as well
as interactions without them being explicitly specified.\cite{Diaz2006}

\begin{figure}
\begin{center} 
\includegraphics[width=.6\textwidth]{figs/RandomForest_10CV_50d.pdf}
\captionsetup{width=\textwidth} 
\caption{Evaluation of RF model predictions ($x$-axis) against an independent
  test set ($y$-axis). The distributions of predicted and empirical
  expression values are shown opposite their
  respective axes. Pearson's correlation coefficient ($r$) and the root
  mean-squared error (RMSE) are also shown (\emph{inset}).
}\label{fig:model}
\end{center} 
\end{figure} 

Figure \ref{fig:model} shows the resulting predictions of a
preliminary RF model against the actual recorded expression over a
test set of approximately 11000 TSS. This model was built with 15
predictors including control ChIP-seq input, though some of these could be
removed without loss of accuracy. The model predictions evluated with
10-fold cross validation show a
significant correlation with measured CAGE levels ($ r = 0.845\pm
1 \times 10^{-4}$; $t_{10868} = 164.4$,
$p < 2 \times 10^{-15}$), and the model is able to explain around
$71\%$ of the variance in the expression response (for comparison a
linear model resulted in $r = 0.825 \pm 3.2 \times 10^{-5}$; $t_{10868} = 152.2$,
$p < 2 \times 10^{-15}$).

This result is worse than that of Dong \emph{et al.}
who achieved cross-validated correlation coefficients of up to $0.9$,
but it is roughly equal to their median test set correlation of
$0.83$.\cite{Dong2012} The RMSEs, when normalised by the range of
observed values, compare more favourably ($0.11$, compared with Dong \emph{et al.}'s: $0.14$). A possible explanation for this decrease in
accuracy is that while both chromatin data and expression timecourse
were measured in H1 hESC cells, the experiments took place at
different institutes and likely using differing protocols and cell cultures. For comparison, a previous study using chromatin
measurements from a number of different sources to predict expression
in a matched cell-type reported a predictive correlation of 0.77.\cite{Karlic2010} Additionally, Dong \emph{et al.} implemented a pseudocount
optimisation step whereby an additional count added to each binned signal
intensity prior to log transformation was optimised to maximise
expression correlation. In the model presented above, a fixed
psuedocount of $1$ was used to avoid introducing positive bias towards
higher correlation. Another difference between the two approaches is
our use of a single-step model; Dong \emph{et al.} found a small
increase in correlation using their classification-regression approach
but with the model implemented herein (Fig. \ref{fig:model}) this approach gave no obvious advantage (for
example, $r
= 0.834 \pm 0.007$, $\textrm{RMSE} = 1.77$ when applied to the same test and
training data used in Fig. \ref{fig:model}).

Having built a reasonable model of $t_0$ expression, the next stage of
this preliminary analysis was to consider successive timepoints. In the
available CD34+ differentiation dataset, this consisted of expression
data recorded at three
timepoints (days 0, 3 and 9---hereafter $t_0$, $t_3$ and $t_9$
respectively). However genome-wide expression
was highly correlated between each of these timepoints (Pearson correlation coefficients: $t_0, t_3 = 0.911; ~t_0,t_9 =
0.913; ~t_3,t_9 = 0.977$), and this high correlation meant that the
genome-wide model performed essentially equally well regardless of the
expression timepoint it was trained or tested on. In future analyses, higher-resolution timecourses may offer
more interesting variation or alternatively genes that remain invariant
throughout the timecourse could be filtered out of the dataset. 

\section{Modelling higher order chromatin}

Accurate predictive modelling of transcription in a variety of cell types offered several novel insights into the internal between histone modifications and transcription factors with transcriptional machinery, and advanced a quantitative explanation of the degree to which correlated features are informative. It is of interest then, to test whether this approach can be applied to other data, such as the reprocessed higher order chromatin data assembled in this work (Chapter 1).

Previous publications have identified several correlates which track compartment eigenvector profiles to varying degrees,\cite{Lieberman2009, Imakaev2012} yet to date these relationships have not been quantitively investigated. The above-described modelling framework offers a statistical approach to understanding the drivers of these observed correlations.

\subsection{Predictive model}

We built Random Forest regression models (see Methods XX) to predict compartment eigenvector profiles genome-wide in three human cell types. Models were found to have high predictive accuracy, with Pearson correlation between predicted and observed compartment eigenvectors in the range of 0.82--0.75 (Fig. \ref{fig:modelres}), comparable to that achieved by Dong \emph{et al.}\cite{Dong2012} in the prediction of transcription.

\begin{figure}
\begin{center} 
\makebox[\textwidth][c]{ 
\includegraphics[width=1.3\textwidth]{figs/modelres.pdf}
}
\captionsetup{width=\textwidth} 
\caption{ {\bf Compartment eigenvector model predictions are highly correlated with observed values.}  Pearson correlation coefficient (PCC) and root mean-squared error (RMSE) report the degree of success of the regression model, whereas accuracy (Acc.) and area under the receiver operating characteristic (AUROC) give the classification accuracy of binarized outcomes.
}\label{fig:modelres}
\end{center} 
\end{figure} 

Our predictive models were also assessed in terms of classification performance, i.e. did the model correctly assign each block to an active "A" or inactive "B" compartment. As with regression, our Random Forest models achieved high classification accuracy with upwards of $80\%$ of the all genomic bins correctly assigned in each cell type (Fig. \ref{fig:modelres}). 

This predictive performance underlines the strong connection between locus-level features and higher order chromatin structure previously noted by \citet{Lieberman2009} Given such highly-predictive models can be generated, it is then of interest to dissect said models in an attempt to understand the nature of this captured relationship.

\subsection{Cross-application}

High predictive accuracy on cell type specific models could be the result of ``over-fitting". In machine-learning, over-fitting refers to the point at which parameters are being optimised to capture noise within a feature set, as well as signal, thereby giving an overoptimistic model performance which would not generalise to another featureset with different noise profiles.

To test if over-fitting was causing our high observed accuracy, we cross-applied models learnt in one cell type to unseen input data from each of the other two cell types under study. If predictive accuracy is a lot lower on unseen data, this lends evidence to the idea that our models may be overfitted to their respective cell types. Conversely, it could be the case that biologically-distinct mechanisms are in place that differ between cell types, preventing a simple cross-application.

\begin{figure}
\begin{center} 
\includegraphics[width=.7\textwidth]{figs/xapp.pdf}
\captionsetup{width=\textwidth} 
\caption{ {\bf Models of higher order chromatin structure learned in one cell type can be cross-applied to two others }
 Each model, trained in one cell type, was applied to the chromatin feature datasets from the other two cell types. (A) The GM12878 model achieved high accuracy when applied to K562 features (PCC $= 0.76$), as did the reciprocal cross (PCC $= 0.75$). (B) In each case, predictive accuracy decreased on cross-application but there remains significant agreement between predicted and empirical values. Acc., accuracy; AUROC, area under the receiver operating characteristic curve; PCC, Pearson correlation coefficient; RMSE, root mean-squared error.
}\label{fig:xapp}
\end{center} 
\end{figure} 

We found cross-application between cell types was possible and with similarly-high levels of accuracy (Fig. \ref{fig:xapp}). This gives good evidence not only that are models are not overfitting to cell-type specific noise, but also that there exist broad rules linking chromatin conformation and locus-level feature aggregation. The cross-application suggests there exists enough commonalities for compartment profile predictions to transcend the cell-type specific biology inherent to an embryonic stem cell or differentiated lymphoblast.

\subsection{Between-cell variability}
 
\begin{figure}
\begin{center} 
\includegraphics[width=.6\textwidth]{figs/stratmod.pdf}
\captionsetup{width=\textwidth} 
\caption{ {\bf Genomic regions that vary across cell types are modelled less successfully than static regions. }
Genome-wide compartment eigenvectors were partitioned into thirds according to their median absolute deviation (MAD) across the three cell types under study. Models were fit independently to each third, and the modelling accuracy is compared.
}\label{fig:stratmod}
\end{center} 
\end{figure} 
 
Given much of the higher order chromatin organisation is conserved between the three cell types used in this work (Fig. \ref{fig:compcor}), a testable hypothesis is that these conserved regions are drivers of cross-applicability between cell types. Conversely, genomic regions which vary most across the cell types in our dataset should be more difficult to predict.

Indeed we found the most variable regions across cell types were then most difficult to predict through our Random Forest modelling framework (Fig. \ref{fig:stratmod}). In each cell type, the third of the genome with the most consistent compartment eigenvectors across cell types could then most accurately be modelled in that cell type, and conversely the most variable third shown significantly depleted predictability (Fig. \ref{fig:stratmod}). This latter result suggests these variable regions could either be those which are noisiest, where the eigenvector is least capturing compartment structure, or where cell-type specific biology is influencing compartment structure in each case, in ways not captured by out input feature set and low resolution modelling pipeline.

\subsection{Variable importance}

\begin{figure}
\begin{center} 
\includegraphics[width=\textwidth]{figs/varimp.pdf}
\captionsetup{width=\textwidth} 
\caption{ {\bf Variable importance per cell type specific model. }
Variable importance for each Random Forest model was measured in terms of percentage increase in mean squared error when permuted (Methods XX) and the top 10 ranking variables are shown for each model.
}\label{fig:varimp}
\end{center} 
\end{figure} 

Having built accurate predictive models, we next dissect the relative variable contributions made from our range of input features and compare these across cell types. An overview on the top 10 most highly-ranked features in cell type specific models shows some agreement but also substantial differences between cell types (Fig. \ref{fig:varimp})

Only one input feature, H3k9me3, is present in the top 10 most important variables of each model (Fig. \ref{fig:top10venn}). H3k9me3 is one of the few features to be negatively correlated with compartment eigenvectors (e.g. Fig. \ref{fig:h1feats}). Of those shared between two cell type models, H3k27me3 is also a repressive mark and deposited by polycomb repressive complex 2 (PRC2)\cite{Vizan2014} while H2A.Z is a histone variant again linked to polycomb-regulated genes and essential for embryonic development.\cite{Creyghton2008} Furthermore EZH2, the catalytic subunit of PRC2,\cite{Deb2014} is also included in the feature set but only highly ranked in the GM12878 cell type model. As another example, MYC and MAX are found in the top 10 influential variables in H1 hESC, while MXI1 is found to be an informative variable in GM12878. This is in keeping with recent results suggesting MYC
binds open chromatin as a transcriptional amplifier in embryonic stem
cells,\cite{Nie2012, Kieffer-Kwon2013a} with MAX and MXI1 long being
known as antagonistic co-regulators of MYC.\cite{Zervos1993} These biological relationships between variables may help explain the observed differences between models: different representatives of correlated clusters of input variables may be being selected in each model (see Section \ref{sec:corrinputs}).

\begin{figure}
\begin{center} 
\includegraphics[width=.5\textwidth]{figs/top10venn.pdf}
\captionsetup{width=\textwidth} 
\caption{ {\bf Intersections of the top 10 ranked variables in the cell type specific models. }
Venn diagram illustrating intersections between sets of ten most influential variables per cell type specific Random Forest regression model of compartment eigenvector (Fig. \ref{fig:varimp}).
}\label{fig:top10venn}
\end{center} 
\end{figure} 

To assess the significance of observed intersections (Fig. \ref{fig:top10venn}), the variable selection process could be modelled with, for example, a multivariate hypergeometric distribution or via simulation. Simulation was used here for simplicity: each intersection was calculated under $10,000$ variables draws with uniform distribution and empirical $p$-values were then calculated accordingly. Under the assumption that variables are ranked independently in each cell type, drawing at least one variable in all three cell types would be expected by chance ($p = 0.6$). Similarly, the overlaps between pairs of cell types is within the range of expectation (probability of 7 or more variables appearing in exactly two sets: $0.39$). Hence these data suggest the top 10 most influential variables are not significantly more alike across the three cell-type specific models than expected by chance, however ten is an arbitrary cutoff, and many of the rankings are based on small differences in variable importance, thus could be unstable between multiple generations of stochastic Random Forest models.

\begin{figure}
\begin{center} 
\makebox[\textwidth][c]{ 
\includegraphics[width=1.2\textwidth]{figs/varimp_diff.pdf}
}
\captionsetup{width=\textwidth} 
\caption{ {\bf Comparison of variable importance between three cell type specific Random Forest models. }
Variable importance for each Random Forest model was measured in terms of percentage increase in mean squared error when permuted (Methods XX). Results are shown sorted by mean variable importance (\emph{left}) and by largest absolute difference in pairwise comparisons (\emph{right}).
}\label{fig:varimp_diff}
\end{center} 
\end{figure} 

In addition to rankings, raw variable importance metrics can be compared between cell-type specific models (Fig. \ref{fig:varimp_diff}). This shows that variables such as CTCF have a relatively small but highly consistent variable importance across the three cell type specific models, whereas other features like ATF3 are highly influential in one cell type but not the other two. Absolute differences in these figures should not be over interpreted and will be affected to some degree by data quality, eigenvector calculation and other sources of noise. Nevertheless there are observations which may reflect biological phenomena, such as the higher relative importance of P300 in both hematopoietic cell line models, potentially reflecting its activity as a histone acetyl transferase that regulates hematopoiesis\cite{Sun2015} and a noted involvement with CTCF in chromatin looping.\cite{Handoko2011}

% figure showing e.g. how individual var regresses against eigen (Egr1compare.pdf)
\begin{figure}
\begin{center} 
\includegraphics[width=\textwidth]{figs/h1feats.pdf}
\captionsetup{width=\textwidth} 
\caption{ {\bf Correlations of individual features with compartment eigenvector in the H1 hESC cell type. }
Two-dimensional kernel density estimates show the density of points in a scatterplot of compartment eigenvector ($x$-axis) against each input feature individually ($y$-axes). Features with a PCC against eigenvector of above or below $0.1$ are coloured as positive or negative, respectively.
}\label{fig:h1feats}
\end{center} 
\end{figure} 


\subsection{Correlating input features}\label{sec:corrinputs}

We have an \emph{a priori} expectation of multicollinearity in our feature set, for example between those that each broadly correlate with transcriptional activity (including POL2, H3K36me3, GC content). To explore these relationships, we performed unsupervised clustering of our feature sets in each cell type (Fig. \ref{fig:feat map}).

\begin{figure}
\begin{center} 
\includegraphics[width=.775\textwidth]{figs/featmap}
\captionsetup{width=\textwidth} 
\caption{ {\bf Correlation heatmaps of the 35 features used to model compartment eigenvectors. }
The Pearson correlation coefficient (PCC) of genome-wide 1 Mb bins of each feature were pairwise correlated with each other. The features were also clustered using hierarchal clustering. The ÒsignificanceÓ of these clusters was determined through multi-scale bootstrap resampling, with those clusters that were stable across different sizes of resampling deemed significant, as implemented in the \texttt{pvclust} R package.\cite{Suzuki2006a}
}\label{fig:featmap}
\end{center} 
\end{figure} 

We found as expected pervasive multicollinearity across our feature sets, with the majority of input variables in each model falling into a persistent "active" cluster containing regions with high DNase hypersensitivity, POL2 binding and histone modifications H3K36me3 as well as GC content (Fig. \ref{fig:feat map}). 

Outliers are also present. H3K9me3, noted for high variable importance in each model (Fig. \ref{fig:varimp}) and the only feature ranked within the top 10 in each model (Fig. \ref{fig:top10venn}) is a clear outgroup in the H1 hESC and GM12878 correlation heatmaps, and in K562 forms a stable cluster only with the P300 transcription factor (Fig. \ref{fig:featmap}). This suggests H3K9me3 is providing orthogonal information to many of the other input variables, and likely explains its high variable importance.

\section{Parsimonious models}

Strongly predictive models can be useful tools to reason about a complex system, however from a researcher's perspective there also exists a trade-off between predictive power and parsimony. Namely simpler models with fewer inputs may be more interpretable and of wider utility, for example in cell types with less ChIP-seq data available than those used in this work. For this reason we explore parsimonious models with reduced feature sets, with an aim to build simpler models of chromatin state while retaining, if possible, similar levels of predictive accuracy.

\subsection{$L_1$ regularisation}

\subsection{Brute force subsampling}

\section{Technical considerations}

\subsection{Resolution}

Thus far models were built at 1 Mb resolution, but if we are capturing true biological relationships we would expect these to hold at higher or lower resolutions. To test this, models leaned at 1 Mb resolution were applied to feature sets binned at 100 kb, an order of magnitude higher resolution.

\begin{figure}
\begin{center} 
\includegraphics[width=.6\textwidth]{figs/100kb.pdf}
\captionsetup{width=\textwidth} 
\caption{ {\bf Models learned at 1 Mb resolution can be applied to higher resolution datasets. }
Despite having been trained on low resolution training sets, the Random Forest models generated can successfully predict compartment eigenvectors at higher resolution (100 kb, a $10\times$ zoom). Eigenvectors at a higher resolution than this do not necessarily reflect A/B compartmentalisation.
}\label{fig:100kb}
\end{center} 
\end{figure} 

Model accuracy when applied to higher resolution input features proved to be similarly high, with empirical PCC being $88$ to $95\%$ as high as that at 1 Mb native resolution (Fig. \ref{fig:100kb}).

Note however, there is some indirect leakage between test and training set when 100 kb bins have been used in aggregate in learning the 1 Mb models. Nevertheless, sustained accuracy is evidence that our models are not resolution-sensitive, and could likely be applied to higher resolutions than the 1 Mb predominantly used in this work.

\subsection{Other modelling approaches}

Random Forest (RF) was \emph{a priori} chosen as an appropriate and powerful modelling tool for this work. Other methods could have been used and should be compared. Here we compare our RF approach with two other options: multiple linear regression and partial least squares regression.

\begin{figure}
\begin{center} 
\includegraphics[width=1.2\textwidth]{figs/diffmethods.pdf}
\captionsetup{width=1.2\textwidth} 
\caption{ {\bf Comparison of Random Forest performance with other modelling approaches. }
Heatmaps show the Pearson correlation coefficient between predicted and observed compartment eigenvectors genome-wide for three regression techniques: multiple linear regression (LM), Random Forest (RF) and partial least squares (PLS). Results are summarised in Table \ref{tab:diffmethods}.
}\label{fig:diffmethods}
\end{center} 
\end{figure} 

Our results confirm RF as a suitable and powerful approach for modelling our relationships of interest in this work (Fig. \ref{fig:diffmethods}), with both the highest cell-type specific performance (PCC between predicted and observed $=0.790$) and on cross-applications (mean PCC $= 0.689$). 

Multiple linear regression assumes linear relationships between model parameters and input features and allows for simple, normally-distributed errors. Surprisingly, this simple approach is capable of accurate cell-type specific predictions (mean PCC $= 0.787$), likely due to the high raw correlation between the inputs and dependent variable. However this simple approach fails to cross-apply between cell types (mean PCC $=0.139$) indicating a problems with overfitting. This can be remedied through variable selection procedures, however a strength of the RF approach is that this step is not necessary, and pre-selection of model variables may result in a sub-optimal end result (ref XX).

Partial least squares regression is another technique which used dimensionality reduction to engineer a lower-dimension orthogonal feature set. Hence this method is well-suited to multi collinear inputs, such as our feature set. As expected, PLS regression provides highly accurate cell type specific predictions (mean PCC $=0.750) and during cross-application (mean PCC $=0.641), though in both cases produces slightly inferior results to RF models (Fig. \ref{fig:diffmethods}).

\begin{table}
\centering
\caption{ {\bf Performance comparison of different modelling techniques. }
 Comparison of mean Pearson correlation coefficient between predicted and observed compartment eigenvectors for three different modelling approaches: LM: linear regression; RF: Random Forest regression; PLS: partial least squares regression. Correlations were averaged per cell type over three cell types (cell type specific) and in the six possible crosses (cross-application) shown in Fig. \ref{fig:diffmethods}.
}
\label{tab:diffmethods}
\begin{tabular}{r|ccc}
& {\bf LM} & {\bf RF} & {\bf PLS} \\
\hline
Cell type specific  &  0.787 & 0.790 & 0.750 \\
Cross-application  & 0.139 & 0.689 & 0.641
\end{tabular}
\end{table}

\subsection{Non-independence}

% use n-1, n-2, ... as predictors for bin N

As recognised through our use of Hidden Markov Models (Methods XX), consecutive bins along a chromosome are non-independent yet thus far predictive models have not considered this inter-dependence. 

This is for two reasons: firstly non independence could be thought of as an artefact of bin-sizing (we have elected to use regular, fixed binning beneath the scale of compartments themselves whereas another approach could use variable bin sizes, for example per compartment, TAD or restriction fragment); secondly using information of a bin's surroundings may obscure by proxy the chromatin features which would otherwise prove predictive. As an example, knowing that bin $x_{i-1}$ and bin $x_{i+1}$ are in compartment state A would allow us with high confidence to predict the state of bin $x_i$, but without learning anything of any region's relationships with their histone modifications and bound factors.

\ifstandalone
\begin{small}
\bibliography{/Users/benmoore/Documents/library,/Users/benmoore/Documents/customrefs}
\end{small}
\fi

\end{document}
